{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "####################################\n",
    "# 1. Generate toy Q/K/V\n",
    "####################################\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "B = 2\n",
    "T = 7\n",
    "D = 16\n",
    "\n",
    "Q_t = torch.randn((B, T, D), dtype=torch.float32)\n",
    "K_t = torch.randn((B, T, D), dtype=torch.float32)\n",
    "V_t = torch.randn((B, T, D), dtype=torch.float32)\n",
    "\n",
    "Q = Q_t.numpy()\n",
    "K = K_t.numpy()\n",
    "V = V_t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c01553",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# 2. Exact attention (reference)\n",
    "####################################\n",
    "scores = Q_t @ K_t.transpose(1, 2) / math.sqrt(D)\n",
    "attn_probs = torch.softmax(scores, dim=-1)\n",
    "out_exact = attn_probs @ V_t\n",
    "out_exact_np = out_exact.numpy()\n",
    "\n",
    "\n",
    "####################################\n",
    "# 3. FlashAttention (PyTorch)\n",
    "####################################\n",
    "def flash_attention_torch(Q, K, V):\n",
    "    Qh = Q.unsqueeze(1)  # (B, 1, T, D)\n",
    "    Kh = K.unsqueeze(1)\n",
    "    Vh = V.unsqueeze(1)\n",
    "    out = torch.nn.functional.scaled_dot_product_attention(Qh, Kh, Vh, dropout_p=0.0)\n",
    "    return out.squeeze(1)\n",
    "\n",
    "\n",
    "out_torch_flash = flash_attention_torch(Q_t, K_t, V_t).numpy()\n",
    "\n",
    "\n",
    "####################################\n",
    "# 4. FlashAttention (NumPy)\n",
    "####################################\n",
    "def flash_attention_numpy(Q, K, V, block_size=4, causal=False):\n",
    "    \"\"\"\n",
    "    FlashAttention-style streaming implementation in NumPy.\n",
    "    Q: [B, Tq, D]\n",
    "    K: [B, Tk, D]\n",
    "    V: [B, Tk, Dv]\n",
    "    \"\"\"\n",
    "    B, Tq, d = Q.shape\n",
    "    _, Tk, dv = V.shape\n",
    "    scale = 1.0 / math.sqrt(d)\n",
    "\n",
    "    O = np.zeros((B, Tq, dv), dtype=np.float32)\n",
    "\n",
    "    for b in range(B):\n",
    "        Qb = Q[b]\n",
    "        Kb = K[b]\n",
    "        Vb = V[b]\n",
    "\n",
    "        # Process queries in blocks\n",
    "        for qi in range(0, Tq, block_size):\n",
    "            q_end = min(qi + block_size, Tq)\n",
    "            Q_blk = Qb[qi:q_end]  # [bq, d]\n",
    "            bq = Q_blk.shape[0]\n",
    "\n",
    "            # Running accumulators (per row)\n",
    "            running_max = np.full(bq, -1e9, dtype=np.float32)\n",
    "            running_sum = np.zeros(bq, dtype=np.float32)  # L accumulator\n",
    "            running_Y = np.zeros((bq, dv), dtype=np.float32)\n",
    "\n",
    "            # Process keys in blocks\n",
    "            for kj in range(0, Tk, block_size):\n",
    "                k_end = min(kj + block_size, Tk)\n",
    "                K_blk = Kb[kj:k_end]  # [bk, d]\n",
    "                V_blk = Vb[kj:k_end]  # [bk, dv]\n",
    "\n",
    "                # Score block: [bq, bk]\n",
    "                S = (Q_blk @ K_blk.T) * scale\n",
    "\n",
    "                # Optional causal mask\n",
    "                if causal:\n",
    "                    qpos = np.arange(qi, q_end)[:, None]\n",
    "                    kpos = np.arange(kj, k_end)[None, :]\n",
    "                    S = np.where(qpos < kpos, -1e9, S)\n",
    "\n",
    "                # Local block max\n",
    "                block_max = S.max(axis=-1)  # [bq]\n",
    "                # Merge into running accumulator (log-sum-exp merge)\n",
    "                M = np.maximum(running_max, block_max)  # new global max\n",
    "                # Compensation factor\n",
    "                compensation_factor = np.exp(running_max - M)\n",
    "\n",
    "                exp_S = np.exp(S - M[:, None])  # [bq, bk]\n",
    "                block_sum = exp_S.sum(axis=-1)  # [bq]\n",
    "                block_Y = exp_S @ V_blk  # [bq, dv]\n",
    "\n",
    "                running_Y = running_Y * compensation_factor[:, None] + block_Y\n",
    "\n",
    "                running_sum = running_sum * compensation_factor + block_sum\n",
    "\n",
    "                running_max = M\n",
    "\n",
    "            # Final output for this block of queries\n",
    "            O[b, qi:q_end] = running_Y / running_sum[:, None]\n",
    "\n",
    "    return O\n",
    "\n",
    "\n",
    "out_numpy_flash = flash_attention_numpy(Q, K, V, block_size=4)\n",
    "\n",
    "####################################\n",
    "# 5. Compare all results\n",
    "####################################\n",
    "print(\n",
    "    \"Max diff (Torch Flash vs Exact):     \",\n",
    "    np.max(np.abs(out_torch_flash - out_exact_np)),\n",
    ")\n",
    "print(\n",
    "    \"Max diff (NumPy Flash vs Exact):     \",\n",
    "    np.max(np.abs(out_numpy_flash - out_exact_np)),\n",
    ")\n",
    "print(\n",
    "    \"Max diff (NumPy Flash vs Torch):     \",\n",
    "    np.max(np.abs(out_numpy_flash - out_torch_flash)),\n",
    ")\n",
    "\n",
    "print(\"\\nExample row (batch0, first 2 rows):\")\n",
    "print(\"Exact:\\n\", out_exact_np[0, :2, :4])\n",
    "print(\"Torch Flash:\\n\", out_torch_flash[0, :2, :4])\n",
    "print(\"NumPy Flash:\\n\", out_numpy_flash[0, :2, :4])\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "\n",
    "block_size = 2\n",
    "q = np.array(\n",
    "    [\n",
    "        [1, 2, 2],\n",
    "        [1, 1, 2],\n",
    "        [1, 2, 1],\n",
    "        [1, 1, 1],\n",
    "        [1, 5, 1],\n",
    "        [3, 1, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "k = np.array(\n",
    "    [\n",
    "        [3, 2, 2],\n",
    "        [3, 1, 2],\n",
    "        [1, 3, 1],\n",
    "        [1, 1, 3],\n",
    "    ]\n",
    ")\n",
    "\n",
    "v = np.array(\n",
    "    [\n",
    "        [1, 2, 4],\n",
    "        [4, 1, 2],\n",
    "        [4, 2, 1],\n",
    "        [1, 1, 4],\n",
    "    ]\n",
    ")\n",
    "\n",
    "```bash\n",
    "#### Exact implementation ####\n",
    ">>> s = np.dot(q, k.T)\n",
    ">>> s\n",
    "array([[11,  9,  9,  9],\n",
    "       [ 9,  8,  6,  8],\n",
    "       [ 9,  7,  8,  6],\n",
    "       [ 7,  6,  5,  5],\n",
    "       [15, 10, 17,  9],\n",
    "       [11, 10,  6,  4]])\n",
    ">>> s_exp = np.exp(s)\n",
    ">>> s_prob = s_exp / s_exp.sum(axis=-1, keepdims=True)\n",
    ">>> s_prob\n",
    "array([[7.11234594e-01, 9.62551353e-02, 9.62551353e-02, 9.62551353e-02],\n",
    "       [5.60052795e-01, 2.06031909e-01, 2.78833868e-02, 2.06031909e-01],\n",
    "       [6.43914260e-01, 8.71443187e-02, 2.36882818e-01, 3.20586033e-02],\n",
    "       [6.10295685e-01, 2.24515236e-01, 8.25945394e-02, 8.25945394e-02],\n",
    "       [1.19072103e-01, 8.02301516e-04, 8.79830446e-01, 2.95150233e-04],\n",
    "       [7.26992890e-01, 2.67445738e-01, 4.89843956e-03, 6.62931706e-04]])\n",
    ">>> np.dot(s_prob, v) # target\n",
    "array([[1.57753081, 1.80748973, 3.51872432],\n",
    "       [1.70174589, 1.58793618, 3.50428602],\n",
    "       [1.97208141, 1.88079708, 3.11506291],\n",
    "       [1.92132933, 1.69289022, 3.30318591],\n",
    "       [3.64189824, 1.99890255, 1.35890406],\n",
    "       [1.81703253, 1.73189133, 3.4504132 ]])\n",
    "\n",
    "#### FlashAttention ####\n",
    ">>> s11 = np.dot(q[:2], k[0:2].T); max11 = s11.max(axis=-1, keepdims=True); M11=np.maximum(0, max11); f11=np.exp(max11-M11); exp11=np.exp(s11-M11); exp_sum11=exp11.sum(axis=-1,keepdims=True); prob11 = exp11 / exp_sum11 + 0*f11;\n",
    ">>> s12 = np.dot(q[:2], k[2:4].T); max12 = s12.max(axis=-1, keepdims=True); M12=np.maximum(M11, max11); f12=np.exp(max12-M12); exp12=np.exp(s12-M12); exp_sum12=exp12.sum(axis=-1,keepdims=True); prob12 = exp12 / exp_sum12 + prob11*f12;\n",
    ">>> s21 = np.dot(q[2:4], k[0:2].T); max21 = s21.max(axis=-1, keepdims=True)\n",
    ">>> s22 = np.dot(q[2:4], k[2:4].T); max22 = s22.max(axis=-1, keepdims=True)\n",
    ">>> s31 = np.dot(q[4:6], k[0:2].T); max31 = s31.max(axis=-1, keepdims=True)\n",
    ">>> s32 = np.dot(q[4:6], k[2:4].T); max32 = s32.max(axis=-1, keepdims=True)\n",
    "\n",
    ">>> s11\n",
    "array([[11,  9],\n",
    "       [ 9,  8]])\n",
    ">>> max11\n",
    "array([11,  9])\n",
    "\n",
    ">>> s12\n",
    "array([[9, 9],\n",
    "       [6, 8]])\n",
    ">>> max12\n",
    "array([9, 8])\n",
    ">>> s21\n",
    "array([[9, 7],\n",
    "       [7, 6]])\n",
    ">>> max22\n",
    "array([8, 5])\n",
    ">>> s31\n",
    "array([[17,  9],\n",
    "       [ 6,  4]])\n",
    ">>> max31\n",
    "array([15, 11])\n",
    ">>> s32\n",
    "array([[17,  9],\n",
    "       [ 6,  4]])\n",
    ">>> max32\n",
    "array([17,  6])\n",
    "\n",
    "\n",
    "running_max = np.full(block_size, -1e9, dtype=np.float32)\n",
    "running_sum = np.zeros(block_size, dtype=np.float32)  # L accumulator\n",
    "running_Y = np.zeros((block_size, v.shape[-1]), dtype=np.float32)\n",
    "O = np.zeros((q.shape[0], v.shape[-1]), dtype=np.float32)\n",
    "\n",
    "V_blk = v[:2]\n",
    "blk1 = np.dot(q[:2], k[:2].T)\n",
    "blk1_max = blk1.max(axis=-1)\n",
    "M = np.maximum(running_max, blk1_max)\n",
    "\n",
    "\n",
    "# Compensation factor\n",
    "compensation_factor = np.exp(running_max - M)\n",
    "\n",
    "exp_blk1 = np.exp(blk1 - M[:, None])  # [bq, bk]\n",
    "block_sum = exp_blk1.sum(axis=-1)  # [bq]\n",
    "block_Y = exp_blk1 @ V_blk  # [bq, dv]\n",
    "\n",
    "running_Y = running_Y * compensation_factor[:, None] + block_Y\n",
    "running_sum = running_sum * compensation_factor + block_sum\n",
    "\n",
    "running_max = M\n",
    "\n",
    "O[0:2, ...] = running_Y / running_sum[:, None]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd87815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment=\"FlashAttention\")\n",
    "dot.attr(rankdir=\"LR\")\n",
    "\n",
    "dot.node(\"Q\", \"Q\\n(L√ód)\", shape=\"box\", style=\"filled\", fillcolor=\"lightblue\")\n",
    "dot.node(\"K\", \"K\\n(L√ód)\", shape=\"box\", style=\"filled\", fillcolor=\"lightgreen\")\n",
    "dot.node(\"QK\", \"QK·µÄ\\n(L√óL)\", shape=\"box\", style=\"filled\", fillcolor=\"lightyellow\")\n",
    "\n",
    "dot.edge(\"Q\", \"QK\")\n",
    "dot.edge(\"QK\", \"K\")\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Q block\n",
    "fig.add_shape(\n",
    "    type=\"rect\", x0=0, y0=0, x1=1, y1=4, line=dict(color=\"blue\"), fillcolor=\"lightblue\"\n",
    ")\n",
    "fig.add_annotation(x=0.5, y=4.3, text=\"Q\", showarrow=False)\n",
    "\n",
    "# K block\n",
    "fig.add_shape(\n",
    "    type=\"rect\",\n",
    "    x0=4,\n",
    "    y0=0,\n",
    "    x1=5,\n",
    "    y1=4,\n",
    "    line=dict(color=\"green\"),\n",
    "    fillcolor=\"lightgreen\",\n",
    ")\n",
    "fig.add_annotation(x=4.5, y=4.3, text=\"K\", showarrow=False)\n",
    "\n",
    "# QK^T block\n",
    "fig.add_shape(\n",
    "    type=\"rect\",\n",
    "    x0=1.5,\n",
    "    y0=0,\n",
    "    x1=3.5,\n",
    "    y1=4,\n",
    "    line=dict(color=\"orange\"),\n",
    "    fillcolor=\"lightyellow\",\n",
    ")\n",
    "fig.add_annotation(x=2.5, y=4.3, text=\"QK<sup>T</sup>\", showarrow=False)\n",
    "\n",
    "fig.update_layout(showlegend=False, xaxis_visible=False, yaxis_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Q vector\n",
    "q_rect = patches.Rectangle(\n",
    "    (0, 0), 1, 4, linewidth=1, edgecolor=\"blue\", facecolor=\"lightblue\"\n",
    ")\n",
    "ax.add_patch(q_rect)\n",
    "ax.text(0.5, 4.3, \"Q\", ha=\"center\", fontsize=12)\n",
    "ax.text(0.5, -0.3, r\"$L \\times d$\", ha=\"center\", fontsize=10)\n",
    "\n",
    "# K vector\n",
    "k_rect = patches.Rectangle(\n",
    "    (4, 0), 1, 4, linewidth=1, edgecolor=\"green\", facecolor=\"lightgreen\"\n",
    ")\n",
    "ax.add_patch(k_rect)\n",
    "ax.text(4.5, 4.3, \"K\", ha=\"center\", fontsize=12)\n",
    "ax.text(4.5, -0.3, r\"$L \\times d$\", ha=\"center\", fontsize=10)\n",
    "\n",
    "# QK^T matrix\n",
    "qk_rect = patches.Rectangle(\n",
    "    (1.5, 0), 2, 4, linewidth=1, edgecolor=\"orange\", facecolor=\"lightyellow\"\n",
    ")\n",
    "ax.add_patch(qk_rect)\n",
    "ax.text(2.5, 4.3, r\"$QK^{\\top}$\", ha=\"center\", fontsize=12)\n",
    "ax.text(2.5, -0.3, r\"$L \\times L$\", ha=\"center\", fontsize=10)\n",
    "\n",
    "# Arrows\n",
    "ax.arrow(1, 2, 0.4, 0, head_width=0.2, head_length=0.1, fc=\"black\", ec=\"black\")\n",
    "ax.arrow(3.5, 2, 0.4, 0, head_width=0.2, head_length=0.1, fc=\"black\", ec=\"black\")\n",
    "\n",
    "ax.set_xlim(-0.5, 5.5)\n",
    "ax.set_ylim(-1, 5)\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9585d2a",
   "metadata": {},
   "source": [
    "Below is a **set of clean, lecture-ready TikZ diagrams** that illustrate the FlashAttention algorithm exactly as explained in:\n",
    "\n",
    "üëâ [https://dev.to/lewis_won/flashattention-by-hand-34im](https://dev.to/lewis_won/flashattention-by-hand-34im)\n",
    "üëâ Using **official TikZ syntax** ([https://tikz.dev](https://tikz.dev))\n",
    "\n",
    "I provide **four diagrams**:\n",
    "\n",
    "1. **Naive Attention (Quadratic Memory)**\n",
    "2. **Tiling Q/K/V into Blocks**\n",
    "3. **Streaming Softmax Logic (m, l updates)**\n",
    "4. **Final Output Assembly**\n",
    "\n",
    "Each diagram is ready for LaTeX slides.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **1. Naive Attention (Full QK·µÄ Materialization)**\n",
    "\n",
    "### *Shows why attention is O(L¬≤) memory.*\n",
    "\n",
    "```latex\n",
    "\\begin{tikzpicture}[>=latex, scale=1.0]\n",
    "\n",
    "% Q vector\n",
    "\\draw[fill=blue!10] (0,0) rectangle (1,4);\n",
    "\\node at (0.5,4.3) {$Q$};\n",
    "\\node at (0.5,-0.3) {$L \\times d$};\n",
    "\n",
    "% K vector\n",
    "\\draw[fill=green!10] (4,0) rectangle (5,4);\n",
    "\\node at (4.5,4.3) {$K$};\n",
    "\\node at (4.5,-0.3) {$L \\times d$};\n",
    "\n",
    "% QK^T matrix\n",
    "\\draw[fill=orange!10] (1.5,0) rectangle (3.5,4);\n",
    "\\node at (2.5,4.3) {$QK^{\\top}$};\n",
    "\\node at (2.5,-0.3) {$L \\times L$};\n",
    "\n",
    "% Arrows\n",
    "\\draw[->, thick] (1,2) -- (1.5,2);\n",
    "\\draw[->, thick] (3.5,2) -- (4,2);\n",
    "\n",
    "\\end{tikzpicture}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **2. FlashAttention: Tiling Q/K/V into Blocks**\n",
    "\n",
    "### *Shows block-processing instead of full matrix.*\n",
    "\n",
    "```latex\n",
    "\\begin{tikzpicture}[>=latex, scale=1.0]\n",
    "\n",
    "% Q blocks\n",
    "\\foreach \\i in {0,1,2} {\n",
    "  \\draw[fill=blue!10] (0,3-\\i*1.3) rectangle (1,4-\\i*1.3);\n",
    "  \\node at (0.5,3.65-\\i*1.3) {$Q_{\\i}$};\n",
    "}\n",
    "\n",
    "% K blocks\n",
    "\\foreach \\i in {0,1,2} {\n",
    "  \\draw[fill=green!10] (2.5,3-\\i*1.3) rectangle (3.5,4-\\i*1.3);\n",
    "  \\node at (3,3.65-\\i*1.3) {$K_{\\i}$};\n",
    "}\n",
    "\n",
    "% V blocks\n",
    "\\foreach \\i in {0,1,2} {\n",
    "  \\draw[fill=red!10] (5,3-\\i*1.3) rectangle (6,4-\\i*1.3);\n",
    "  \\node at (5.5,3.65-\\i*1.3) {$V_{\\i}$};\n",
    "}\n",
    "\n",
    "% Arrows\n",
    "\\foreach \\i in {0,1,2} {\n",
    "  \\draw[->, thick] (1,3.5-\\i*1.3) -- (2.5,3.5-\\i*1.3);\n",
    "  \\draw[->, thick] (3.5,3.5-\\i*1.3) -- (5,3.5-\\i*1.3);\n",
    "}\n",
    "\n",
    "\\node at (3,-1) {\\Large Blocked Processing Instead of Full $QK^\\top$};\n",
    "\n",
    "\\end{tikzpicture}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **3. Streaming Softmax (Online Softmax)**\n",
    "\n",
    "### *The key algorithm of FlashAttention.*\n",
    "\n",
    "This diagram illustrates how each block updates:\n",
    "\n",
    "* ( m ) = running max\n",
    "* ( l ) = running softmax denominator\n",
    "* contributions to output ( O )\n",
    "\n",
    "```latex\n",
    "\\begin{tikzpicture}[>=latex, scale=1.1]\n",
    "\n",
    "% Block scores Sb\n",
    "\\draw[fill=orange!20] (0,0) rectangle (3,1);\n",
    "\\node at (1.5,0.5) {$S_b = Q_b K_b^{\\top}$};\n",
    "\n",
    "% Running max m_old -> m_new\n",
    "\\draw[->, thick] (3,0.5) -- (4,0.5);\n",
    "\\node at (4.7,0.5) {$m_{\\text{new}} = \\max(m_{\\text{old}}, \\max S_b)$};\n",
    "\n",
    "% Running denominator\n",
    "\\draw[fill=blue!10] (0,-1.5) rectangle (3,-0.5);\n",
    "\\node at (1.5,-1) {$l_{\\text{old}}$};\n",
    "\n",
    "\\draw[->, thick] (3,-1) -- (4,-1);\n",
    "\\node at (6,-1)\n",
    "{\n",
    "$\\displaystyle\n",
    "l_{\\text{new}} = e^{m_{\\text{old}} - m_{\\text{new}}} l_{\\text{old}}\n",
    "+ \\sum e^{S_b - m_{\\text{new}}}\n",
    "$\n",
    "};\n",
    "\n",
    "% Probabilities\n",
    "\\draw[fill=green!10] (0,-3) rectangle (3,-2);\n",
    "\\node at (1.5,-2.5)\n",
    "{\n",
    "$\\displaystyle\n",
    "P_b = \\frac{e^{S_b - m_{\\text{new}}}}{l_{\\text{new}}}\n",
    "$\n",
    "};\n",
    "\n",
    "% Output accumulation\n",
    "\\draw[->, thick] (3,-2.5) -- (4,-2.5);\n",
    "\\node at (6,-2.5)\n",
    "{\n",
    "$\\displaystyle\n",
    "O \\;+=\\; P_b V_b\n",
    "$\n",
    "};\n",
    "\n",
    "\\end{tikzpicture}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **4. Final Assembly of Attention Output**\n",
    "\n",
    "### *Combining block contributions into the final O.*\n",
    "\n",
    "```latex\n",
    "\\begin{tikzpicture}[>=latex, scale=1.0]\n",
    "\n",
    "% Outputs\n",
    "\\foreach \\i in {0,1,2} {\n",
    "  \\draw[fill=purple!10] (0,3-\\i*1.3) rectangle (2,4-\\i*1.3);\n",
    "  \\node at (1,3.65-\\i*1.3) {$P_{\\i} V_{\\i}$};\n",
    "}\n",
    "\n",
    "\\draw[->, thick] (2,3.5) -- (3,2);\n",
    "\\draw[->, thick] (2,2.2) -- (3,2);\n",
    "\\draw[->, thick] (2,0.9) -- (3,2);\n",
    "\n",
    "% Final sum\n",
    "\\draw[fill=purple!30] (3,1.5) rectangle (5,2.5);\n",
    "\\node at (4,2) {$O = \\sum_b P_b V_b$};\n",
    "\n",
    "\\end{tikzpicture}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üìò **These diagrams can be used directly in Beamer slides**\n",
    "\n",
    "Example wrapper:\n",
    "\n",
    "```latex\n",
    "\\begin{frame}{FlashAttention: Streaming Softmax}\n",
    "\\centering\n",
    "\\input{streaming_softmax_diagram.tex}\n",
    "\\end{frame}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üéÅ **Would you like more diagrams?**\n",
    "\n",
    "I can add:\n",
    "\n",
    "* KV-cache growth over time\n",
    "* Rotary embedding visualization\n",
    "* GQA head grouping diagram\n",
    "* Memory comparison: Naive vs FlashAttention\n",
    "* Inference vs Training attention computation paths\n",
    "\n",
    "Just tell me what you want for your lecture notes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86c9a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
