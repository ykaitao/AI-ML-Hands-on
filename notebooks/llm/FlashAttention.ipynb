{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a7c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "####################################\n",
    "# 1. Generate toy Q/K/V\n",
    "####################################\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "B = 2\n",
    "T = 7\n",
    "D = 16\n",
    "\n",
    "Q_t = torch.randn((B, T, D), dtype=torch.float32)\n",
    "K_t = torch.randn((B, T, D), dtype=torch.float32)\n",
    "V_t = torch.randn((B, T, D), dtype=torch.float32)\n",
    "\n",
    "Q = Q_t.numpy()\n",
    "K = K_t.numpy()\n",
    "V = V_t.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c01553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff (Torch Flash vs Exact):      1.7881393e-07\n",
      "Max diff (NumPy Flash vs Exact):      2.3841858e-07\n",
      "Max diff (NumPy Flash vs Torch):      2.3841858e-07\n",
      "\n",
      "Example row (batch0, first 2 rows):\n",
      "Exact:\n",
      " [[-0.20096852 -0.5869937  -0.05182338 -0.4397468 ]\n",
      " [-0.1666379  -0.5908006  -0.6343283  -0.5527998 ]]\n",
      "Torch Flash:\n",
      " [[-0.20096853 -0.5869937  -0.05182336 -0.4397468 ]\n",
      " [-0.1666379  -0.5908006  -0.63432837 -0.5527999 ]]\n",
      "NumPy Flash:\n",
      " [[-0.20096852 -0.58699363 -0.05182335 -0.43974683]\n",
      " [-0.16663791 -0.5908006  -0.63432837 -0.5527999 ]]\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# 2. Exact attention (reference)\n",
    "####################################\n",
    "scores = Q_t @ K_t.transpose(1, 2) / math.sqrt(D)\n",
    "attn_probs = torch.softmax(scores, dim=-1)\n",
    "out_exact = attn_probs @ V_t\n",
    "out_exact_np = out_exact.numpy()\n",
    "\n",
    "\n",
    "####################################\n",
    "# 3. FlashAttention (PyTorch)\n",
    "####################################\n",
    "def flash_attention_torch(Q, K, V):\n",
    "    Qh = Q.unsqueeze(1)  # (B, 1, T, D)\n",
    "    Kh = K.unsqueeze(1)\n",
    "    Vh = V.unsqueeze(1)\n",
    "    out = torch.nn.functional.scaled_dot_product_attention(Qh, Kh, Vh, dropout_p=0.0)\n",
    "    return out.squeeze(1)\n",
    "\n",
    "\n",
    "out_torch_flash = flash_attention_torch(Q_t, K_t, V_t).numpy()\n",
    "\n",
    "\n",
    "####################################\n",
    "# 4. FlashAttention (NumPy)\n",
    "####################################\n",
    "def flash_attention_numpy(Q, K, V, block_size=4, causal=False):\n",
    "    \"\"\"\n",
    "    FlashAttention-style streaming implementation in NumPy.\n",
    "    Q: [B, Tq, D]\n",
    "    K: [B, Tk, D]\n",
    "    V: [B, Tk, Dv]\n",
    "    \"\"\"\n",
    "    B, Tq, d = Q.shape\n",
    "    _, Tk, dv = V.shape\n",
    "    scale = 1.0 / math.sqrt(d)\n",
    "\n",
    "    O = np.zeros((B, Tq, dv), dtype=np.float32)\n",
    "\n",
    "    for b in range(B):\n",
    "        Qb = Q[b]\n",
    "        Kb = K[b]\n",
    "        Vb = V[b]\n",
    "\n",
    "        # Process queries in blocks\n",
    "        for qi in range(0, Tq, block_size):\n",
    "            q_end = min(qi + block_size, Tq)\n",
    "            Q_blk = Qb[qi:q_end]  # [bq, d]\n",
    "            bq = Q_blk.shape[0]\n",
    "\n",
    "            # Running accumulators (per row)\n",
    "            running_max = np.full(bq, -1e9, dtype=np.float32)\n",
    "            running_sum = np.zeros(bq, dtype=np.float32)  # L accumulator\n",
    "            running_Y = np.zeros((bq, dv), dtype=np.float32)\n",
    "\n",
    "            # Process keys in blocks\n",
    "            for kj in range(0, Tk, block_size):\n",
    "                k_end = min(kj + block_size, Tk)\n",
    "                K_blk = Kb[kj:k_end]  # [bk, d]\n",
    "                V_blk = Vb[kj:k_end]  # [bk, dv]\n",
    "\n",
    "                # Score block: [bq, bk]\n",
    "                S = (Q_blk @ K_blk.T) * scale\n",
    "\n",
    "                # Optional causal mask\n",
    "                if causal:\n",
    "                    qpos = np.arange(qi, q_end)[:, None]\n",
    "                    kpos = np.arange(kj, k_end)[None, :]\n",
    "                    S = np.where(qpos < kpos, -1e9, S)\n",
    "\n",
    "                # Local block max\n",
    "                block_max = S.max(axis=-1)  # [bq]\n",
    "                # Merge into running accumulator (log-sum-exp merge)\n",
    "                M = np.maximum(running_max, block_max)  # new global max\n",
    "                # Compensation factor\n",
    "                compensation_factor = np.exp(running_max - M)\n",
    "\n",
    "                exp_S = np.exp(S - M[:, None])  # [bq, bk]\n",
    "                block_sum = exp_S.sum(axis=-1)  # [bq]\n",
    "                block_Y = exp_S @ V_blk  # [bq, dv]\n",
    "\n",
    "                running_Y = running_Y * compensation_factor[:, None] + block_Y\n",
    "\n",
    "                running_sum = running_sum * compensation_factor + block_sum\n",
    "\n",
    "                running_max = M\n",
    "\n",
    "            # Final output for this block of queries\n",
    "            O[b, qi:q_end] = running_Y / running_sum[:, None]\n",
    "\n",
    "    return O\n",
    "\n",
    "\n",
    "out_numpy_flash = flash_attention_numpy(Q, K, V, block_size=4)\n",
    "\n",
    "####################################\n",
    "# 5. Compare all results\n",
    "####################################\n",
    "print(\n",
    "    \"Max diff (Torch Flash vs Exact):     \",\n",
    "    np.max(np.abs(out_torch_flash - out_exact_np)),\n",
    ")\n",
    "print(\n",
    "    \"Max diff (NumPy Flash vs Exact):     \",\n",
    "    np.max(np.abs(out_numpy_flash - out_exact_np)),\n",
    ")\n",
    "print(\n",
    "    \"Max diff (NumPy Flash vs Torch):     \",\n",
    "    np.max(np.abs(out_numpy_flash - out_torch_flash)),\n",
    ")\n",
    "\n",
    "print(\"\\nExample row (batch0, first 2 rows):\")\n",
    "print(\"Exact:\\n\", out_exact_np[0, :2, :4])\n",
    "print(\"Torch Flash:\\n\", out_torch_flash[0, :2, :4])\n",
    "print(\"NumPy Flash:\\n\", out_numpy_flash[0, :2, :4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
